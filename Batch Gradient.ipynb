{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661ab218-2ef0-48e6-b070-db394b53bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 3000 samples\n",
      "Validation data: 1000 samples\n",
      "Test data: 1000 samples\n",
      "Model initialized with 10 features and 3 classes\n",
      "Starting training...\n",
      "Epoch    0 | Train Loss: 1.0960 | Train Acc: 0.3877 | Val Loss: 1.0290 | Val Acc: 0.5960\n",
      "Epoch   50 | Train Loss: 0.8325 | Train Acc: 0.6093 | Val Loss: 0.8199 | Val Acc: 0.6360\n",
      "Epoch  100 | Train Loss: 0.8169 | Train Acc: 0.6277 | Val Loss: 0.8076 | Val Acc: 0.6450\n",
      "Epoch  150 | Train Loss: 0.8135 | Train Acc: 0.6363 | Val Loss: 0.8054 | Val Acc: 0.6450\n",
      "Epoch  200 | Train Loss: 0.8126 | Train Acc: 0.6390 | Val Loss: 0.8050 | Val Acc: 0.6440\n",
      "Early stopping at epoch 220 - no improvement for 10 epochs\n",
      "\n",
      "Training complete! Testing on unseen data...\n",
      "Final Test Accuracy: 0.6410 (64.1%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: CREATE AND PREPARE THE DATA\n",
    "# ==============================================================================\n",
    "\n",
    "# Create a synthetic dataset for testing\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,    # Total number of data points\n",
    "    n_features=10,     # Number of input features\n",
    "    n_classes=3,       # Number of classes to predict (0, 1, or 2)\n",
    "    n_informative=6,   # Number of features that actually help with classification\n",
    "    random_state=42    # For reproducible results\n",
    ")\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "# Example: if y = [0, 1, 2], one-hot becomes [[1,0,0], [0,1,0], [0,0,1]]\n",
    "num_classes = y.max() + 1\n",
    "num_samples = y.size\n",
    "y_onehot = np.zeros((num_samples, num_classes))\n",
    "y_onehot[np.arange(num_samples), y] = 1\n",
    "\n",
    "# Split data: 60% training, 20% validation, 20% testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_onehot, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation data: {X_val.shape[0]} samples\") \n",
    "print(f\"Test data: {X_test.shape[0]} samples\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: DEFINE HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def softmax(raw_scores):\n",
    "    \"\"\",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
    "    Convert raw scores to probabilities that sum to 1.\n",
    "    \n",
    "    Why subtract the max? For numerical stability!\n",
    "    Large numbers in exp() can cause overflow, so we subtract the max\n",
    "    to keep numbers manageable without changing the final probabilities.\n",
    "    \"\"\"\n",
    "    # Subtract max to prevent overflow (numerical stability trick)\n",
    "    stable_scores = raw_scores - np.max(raw_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    # Convert to probabilities using exponential\n",
    "     #Mathematical Formula: softmax(z_i) = exp(z_i) / Σ(exp(z_j))\n",
    "\n",
    "    exp_scores = np.exp(stable_scores)\n",
    "    probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "   \n",
    "\n",
    "def calculate_loss(true_labels, predicted_probs):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss: measures how far our predictions are from the truth.\n",
    "    Lower loss = better predictions.\n",
    "    \n",
    "    The math: -sum(true_label * log(predicted_probability)) / number_of_samples\n",
    "    We add a tiny epsilon (1e-9) to avoid taking log of zero.\n",
    "    \"\"\"\n",
    "    num_samples = true_labels.shape[0]\n",
    "    epsilon = 1e-9  # Prevent log(0) which would be -infinity\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    #L = -1/N × Σ(y_i × log(ŷ_i))\n",
    "\n",
    "    loss = -np.sum(true_labels * np.log(predicted_probs + epsilon)) / num_samples\n",
    "    return loss\n",
    "\n",
    "def calculate_accuracy(true_labels, predicted_probs):\n",
    "    \"\"\"Calculate what percentage of predictions were correct.\"\"\"\n",
    "    # Convert probabilities back to class predictions\n",
    "    true_classes = np.argmax(true_labels, axis=1)\n",
    "    predicted_classes = np.argmax(predicted_probs, axis=1)\n",
    "    \n",
    "    # Calculate percentage of correct predictions\n",
    "    #Accuracy = (1/N) × Σ(I(y_i = ŷ_i))\n",
    "\n",
    "    accuracy = np.mean(true_classes == predicted_classes)\n",
    "    return accuracy\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: INITIALIZE THE MODEL PARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "# Our model is: predictions = softmax(X * weights + bias)\n",
    "num_features = X_train.shape[1]  # 10 features\n",
    "num_classes = y_train.shape[1]   # 3 classes\n",
    "\n",
    "# Initialize weights randomly (small values work better)\n",
    "weights = np.random.randn(num_features, num_classes) * 0.01\n",
    "\n",
    "# Initialize bias to zero\n",
    "bias = np.zeros((1, num_classes))\n",
    "\n",
    "print(f\"Model initialized with {num_features} features and {num_classes} classes\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: SET TRAINING HYPERPARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "learning_rate = 0.1      # How big steps to take when updating weights\n",
    "max_epochs = 1000        # Maximum number of training iterations\n",
    "patience = 10            # Stop early if no improvement for this many epochs\n",
    "\n",
    "# Variables for early stopping\n",
    "best_validation_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_weights = None\n",
    "best_bias = None\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 5: TRAINING LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    # === FORWARD PASS: Make predictions ===\n",
    "    # Calculate raw scores: X * weights + bias\n",
    "    raw_scores = np.dot(X_train, weights) + bias\n",
    "    \n",
    "    # Convert scores to probabilities using softmax\n",
    "    predicted_probs = softmax(raw_scores)\n",
    "    \n",
    "    # Calculate how wrong we are (loss)\n",
    "    training_loss = calculate_loss(y_train, predicted_probs)\n",
    "    \n",
    "    # === BACKWARD PASS: Calculate gradients ===\n",
    "    # This is where we figure out how to adjust weights to reduce loss\n",
    "    \n",
    "    num_training_samples = X_train.shape[0]\n",
    "    \n",
    "    # Gradient for weights: how much each weight contributed to the error\n",
    "    # Math: X^T * (predicted - actual) / num_samples\n",
    "    error = predicted_probs - y_train\n",
    "    weight_gradient = np.dot(X_train.T, error) / num_training_samples\n",
    "    \n",
    "    # Gradient for bias: average error across all samples\n",
    "    bias_gradient = np.sum(error, axis=0, keepdims=True) / num_training_samples\n",
    "    \n",
    "    # === UPDATE PARAMETERS ===\n",
    "    # Move weights in the opposite direction of the gradient\n",
    "    weights = weights - learning_rate * weight_gradient\n",
    "    bias = bias - learning_rate * bias_gradient\n",
    "    \n",
    "    # === VALIDATION CHECK ===\n",
    "    # Check performance on validation set (data model hasn't seen during training)\n",
    "    val_raw_scores = np.dot(X_val, weights) + bias\n",
    "    val_predicted_probs = softmax(val_raw_scores)\n",
    "    validation_loss = calculate_loss(y_val, val_predicted_probs)\n",
    "    \n",
    "    # === EARLY STOPPING LOGIC ===\n",
    "    # If validation loss improves, save the model\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        best_weights = weights.copy()  # Save best weights\n",
    "        best_bias = bias.copy()        # Save best bias\n",
    "        patience_counter = 0           # Reset patience\n",
    "    else:\n",
    "        # If no improvement, increment patience counter\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} - no improvement for {patience} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Print progress every 50 epochs\n",
    "    if epoch % 50 == 0:\n",
    "        train_acc = calculate_accuracy(y_train, predicted_probs)\n",
    "        val_acc = calculate_accuracy(y_val, val_predicted_probs)\n",
    "        print(f\"Epoch {epoch:4d} | Train Loss: {training_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {validation_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 6: TEST THE FINAL MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nTraining complete! Testing on unseen data...\")\n",
    "\n",
    "# Use the best weights (from validation) to make final predictions\n",
    "test_raw_scores = np.dot(X_test, best_weights) + best_bias\n",
    "test_predicted_probs = softmax(test_raw_scores)\n",
    "test_accuracy = calculate_accuracy(y_test, test_predicted_probs)\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92dd14c-8f59-4e74-b44f-1b89011fd03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
